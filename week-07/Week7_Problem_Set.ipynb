{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ye5YFtvToNe"
      },
      "source": [
        "# COMP0189: Applied Artificial Intelligence\n",
        "## Week 7 (Model Interpretation and Feature selection)\n",
        "\n",
        "\n",
        "## Learning goals üéØ\n",
        "1. Learn how to properly implement feature selection to avoid leaking information.\n",
        "2. Learn how to use different strategies for interpreting machine learning models.\n",
        "\n",
        "### Acknowledgements\n",
        "- https://scikit-learn.org/stable/\n",
        "- https://scikit-learn.org/stable/auto_examples/inspection/plot_linear_model_coefficient_interpretation.html#id1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Zp3mpVv4A9_W"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import scipy as sp\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jbHJ5b5A9_Y"
      },
      "source": [
        "# Part 1: A common error: leaking information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AldmQ19uA9_Y"
      },
      "source": [
        "We will start with a toy example to illustrate a common mistake when using feature selection. We will create a random dataset with 10.000 features and 100 samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MAm-RPVBA9_Z"
      },
      "outputs": [],
      "source": [
        "rnd = np.random.RandomState(seed=0)\n",
        "X = rnd.normal(size=(100, 10000))\n",
        "X_test = rnd.normal(size=(100, 10000))\n",
        "y = rnd.normal(size=(100,))\n",
        "y_test = rnd.normal(size=(100,))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JPA5RNRA9_Z",
        "outputId": "7db65bb3-ab34-4925-9408-7cdcadf70126"
      },
      "outputs": [],
      "source": [
        "print(X.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RC11ZosYA9_a"
      },
      "source": [
        "We might consider that 10.000 is a very high number of features and that we need to use feature selection. So, let's select the 5% most informative features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XT3sN3KeA9_a",
        "outputId": "b3936f72-1fe1-4abb-9278-20f8932d7543"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import SelectPercentile, f_regression\n",
        "from sklearn.linear_model import RidgeCV\n",
        "\n",
        "select = SelectPercentile(score_func=f_regression,\n",
        "                          percentile=5)\n",
        "select.fit(X, y)\n",
        "X_sel = select.transform(X)\n",
        "\n",
        "print(X_sel.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlJXXsIeA9_b"
      },
      "source": [
        "Now we will create a pipeline to pre-process the data and fit a regression model to see if we can predict the random labels from the selected features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSntGdZ8A9_b",
        "outputId": "f55f77ff-6be4-4bb2-bade-0b79f3fb66fe"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_sel, y, random_state=0)\n",
        "pipe = make_pipeline(StandardScaler(), Ridge())\n",
        "pipe.fit(X_train, y_train)\n",
        "pipe.score(X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tas54WLSA9_c"
      },
      "source": [
        "These are great results but how did we get such good results on a random dataset?\n",
        "\n",
        "These results are due to information leaking."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qk97Om8OA9_c"
      },
      "source": [
        "### Task 1: Implement a correct pipeline to pre-process the data, select the top 5% features and train a regression model to predict th random labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxqz_phZA9_c",
        "outputId": "88c78ea7-53ec-40bf-8819-99232e03eeea"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(None)\n",
        "pipe = make_pipeline(None, select,  Ridge())\n",
        "pipe.fit(None)\n",
        "pipe.score(None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pCcvR8DA9_d"
      },
      "source": [
        "These results make more sense from what we would expet with random labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBrEYQUbA9_d"
      },
      "source": [
        "# Part 2: Model interpretation and feature selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1_HOv7dA9_e"
      },
      "source": [
        "\n",
        "For this part we will use data from the ‚ÄúCurrent Population Survey‚Äù from 1985 to predict wage as a function of various features such as experience, age, or education."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZhuWH7aToNj"
      },
      "source": [
        "We fetch the data from OpenML. Note that setting the parameter as_frame to True will retrieve the data as a pandas dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bbrRMTOToNk",
        "outputId": "d9f49e2a-3ed9-4c71-aca0-ce0edbbce6c3",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "survey = fetch_openml(data_id=534, as_frame=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssZKhHxZToNk"
      },
      "source": [
        "Now we identify features X and targets y: the column WAGE is our target variable (i.e., the variable which we want to predict)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "Vp3beQbEToNl",
        "outputId": "3c5e8bc0-9894-4c61-e817-ea776b6f25e9",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "X = survey.data[survey.feature_names]\n",
        "X.describe(include=\"all\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJWJ_i1rToNl"
      },
      "source": [
        "Note that the dataset contains categorical and numerical variables. We will need to take this into account when preprocessing the dataset thereafter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "44iBUDR6ToNm",
        "outputId": "48cd5f7c-18ff-47de-83a5-d3cd4cd2ea0c"
      },
      "outputs": [],
      "source": [
        "X.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgOH32mFToNm"
      },
      "source": [
        "Our target for prediction: the wage. Wages are described as floating-point number in dollars per hour.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEv3tul5ToNm",
        "outputId": "03978c38-6f06-47dc-dc8a-e4bd6f39c4ff",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "y = survey.target.values.ravel()\n",
        "survey.target.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtFe0csGToNn"
      },
      "source": [
        "We now split the sample into a train and a test dataset. Only the train dataset will be used in the following exploratory analysis. This is a way to emulate a real situation where predictions are performed on an unknown target, and we don‚Äôt want our analysis and decisions to be biased by our knowledge of the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "IajsRtoNToNn"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgEMsTCKToNo"
      },
      "source": [
        "First, let‚Äôs get some insights by looking at the variable distributions and at the pairwise relationships between them. Only numerical variables will be used. In the following plot, each dot represents a sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zY0615zHToNo",
        "outputId": "39de725b-0284-4e4d-bffc-b1155fb6b81b"
      },
      "outputs": [],
      "source": [
        "train_dataset = X_train.copy()\n",
        "train_dataset.insert(0, \"WAGE\", y_train)\n",
        "_ = sns.pairplot(train_dataset, kind=\"reg\", diag_kind=\"kde\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oe6HBlS8ToNp"
      },
      "source": [
        "Looking closely at the WAGE distribution reveals that it has a long tail. For this reason, we should take its logarithm to turn it approximately into a normal distribution (linear models such as ridge or lasso work best for a normal distribution of error).\n",
        "\n",
        "The WAGE is increasing when EDUCATION is increasing. Note that the dependence between WAGE and EDUCATION represented here is a marginal dependence, i.e., it describes the behavior of a specific variable without keeping the others fixed.\n",
        "\n",
        "Also, the EXPERIENCE and AGE are strongly linearly correlated.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kr3AqplYToNp"
      },
      "source": [
        "Before design a machine learning pipeline, we should check the type of data that we are dealing with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_maegsSToNp",
        "outputId": "e7a848fe-b135-4e85-8351-5af8ce5fb953"
      },
      "outputs": [],
      "source": [
        "survey.data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1ljuZ0oToNp"
      },
      "source": [
        "As seen previously, the dataset contains columns with different data types and we need to apply a specific preprocessing for each data types."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qt-dOJIVToNq"
      },
      "source": [
        "## Task 2: Implement a machine learning pipeline that includes pre-processing and cross-validation to optimize the models hyperparameters and use the pipeline with rigde regression, Lasso and elastic-net regression regression to predict the wages from the other features.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "8GSF28gHToNq",
        "outputId": "a451e655-652f-44ad-e32c-030ae79de56b"
      },
      "outputs": [],
      "source": [
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.preprocessing import OneHotEncoder,StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.compose import TransformedTargetRegressor\n",
        "from sklearn.linear_model import Ridge,RidgeCV, LassoCV\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "categorical_columns = [\"RACE\", \"OCCUPATION\", \"SECTOR\", \"MARR\", \"UNION\", \"SEX\", \"SOUTH\"]\n",
        "numerical_columns = [\"EDUCATION\", \"EXPERIENCE\", \"AGE\"]\n",
        "\n",
        "preprocessor = make_column_transformer(\n",
        "    (None, categorical_columns),\n",
        "   (None, numerical_columns),\n",
        "    verbose_feature_names_out=False,\n",
        ")\n",
        "\n",
        "\n",
        "alphas = np.logspace(-10, 10, 21)  # alpha values to be chosen from by cross-validation\n",
        "\n",
        "model_Ridge= make_pipeline(\n",
        "    None,\n",
        "    TransformedTargetRegressor(\n",
        "        regressor=None,\n",
        "        func=np.log10,\n",
        "        inverse_func=sp.special.exp10,\n",
        "    ),\n",
        ")\n",
        "model_Ridge.fit(None)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYlRg6LLE8Jh",
        "outputId": "3663ff2f-6cdf-4864-84b5-a2ee736b5802"
      },
      "outputs": [],
      "source": [
        "model_Ridge[-1].regressor_.alpha_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "UnfxxYoJA9_l",
        "outputId": "15947245-cfa0-4264-e2ce-89fb56359851"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LassoCV\n",
        "\n",
        "alphas = np.logspace(-10, 10, 21)  # alpha values to be chosen from by cross-validation\n",
        "\n",
        "model_Lasso= make_pipeline(\n",
        "    None,\n",
        "    TransformedTargetRegressor(\n",
        "        regressor=None,\n",
        "        func=np.log10,\n",
        "        inverse_func=sp.special.exp10,\n",
        "    ),\n",
        ")\n",
        "model_Lasso.fit(None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFzD3bvqA9_m",
        "outputId": "6e86b745-17cf-4f78-e129-cda3960ccc89"
      },
      "outputs": [],
      "source": [
        "model_Lasso[-1].regressor_.alpha_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "ATCq14A_A9_m",
        "outputId": "9d36acb2-c892-4448-f1a7-102815c16043"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import ElasticNetCV\n",
        "\n",
        "alphas = np.logspace(-10, 10, 21)  # alpha values to be chosen from by cross-validation\n",
        "\n",
        "model_EN= make_pipeline(\n",
        "    preprocessor,\n",
        "    TransformedTargetRegressor(\n",
        "        regressor=None,\n",
        "        func=np.log10,\n",
        "        inverse_func=sp.special.exp10,\n",
        "    ),\n",
        ")\n",
        "model_EN.fit(None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHIdSIMNA9_m",
        "outputId": "2d8c7059-2638-4538-c810-6a1070d2df29"
      },
      "outputs": [],
      "source": [
        "model_EN[-1].regressor_.alpha_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_h3fwRXYToNr"
      },
      "source": [
        "### Task 2.1 Check the performance of the computed models plotting its predictions on the test set and computing the median absolute error of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "IQFz3MfOToNr",
        "outputId": "3a90b2e8-9207-4eee-99fd-5c29e679cd90"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import PredictionErrorDisplay, median_absolute_error\n",
        "\n",
        "mae_train = median_absolute_error(None)\n",
        "y_pred = None\n",
        "mae_test = median_absolute_error(None)\n",
        "scores = {\n",
        "    \"MedAE on training set\": f\"{mae_train:.2f} $/hour\",\n",
        "    \"MedAE on testing set\": f\"{mae_test:.2f} $/hour\",\n",
        "}\n",
        "\n",
        "_, ax = plt.subplots(figsize=(5, 5))\n",
        "display = PredictionErrorDisplay.from_predictions(\n",
        "    y_test, y_pred, kind=\"actual_vs_predicted\", ax=ax, scatter_kwargs={\"alpha\": 0.5}\n",
        ")\n",
        "ax.set_title(\"Ridge model, optimum regularization\")\n",
        "for name, score in scores.items():\n",
        "    ax.plot([], [], \" \", label=f\"{name}: {score}\")\n",
        "ax.legend(loc=\"upper left\")\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "slimCjIyA9_n",
        "outputId": "70b5de84-1eb9-4113-9c7b-8a58eaf9487c"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import PredictionErrorDisplay, median_absolute_error\n",
        "\n",
        "mae_train = median_absolute_error(None)\n",
        "y_pred = None\n",
        "mae_test = median_absolute_error(None)\n",
        "scores = {\n",
        "    \"MedAE on training set\": f\"{mae_train:.2f} $/hour\",\n",
        "    \"MedAE on testing set\": f\"{mae_test:.2f} $/hour\",\n",
        "}\n",
        "\n",
        "_, ax = plt.subplots(figsize=(5, 5))\n",
        "display = PredictionErrorDisplay.from_predictions(\n",
        "    y_test, y_pred, kind=\"actual_vs_predicted\", ax=ax, scatter_kwargs={\"alpha\": 0.5}\n",
        ")\n",
        "ax.set_title(\"Lasso model, optimum regularization\")\n",
        "for name, score in scores.items():\n",
        "    ax.plot([], [], \" \", label=f\"{name}: {score}\")\n",
        "ax.legend(loc=\"upper left\")\n",
        "plt.tight_layout()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "YtqVVlssA9_o",
        "outputId": "5f3f0519-8c33-45bb-ba74-36562a85a81e"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import PredictionErrorDisplay, median_absolute_error\n",
        "\n",
        "mae_train = median_absolute_error(None)\n",
        "y_pred = None\n",
        "mae_test = median_absolute_error(None)\n",
        "scores = {\n",
        "    \"MedAE on training set\": f\"{mae_train:.2f} $/hour\",\n",
        "    \"MedAE on testing set\": f\"{mae_test:.2f} $/hour\",\n",
        "}\n",
        "\n",
        "_, ax = plt.subplots(figsize=(5, 5))\n",
        "display = PredictionErrorDisplay.from_predictions(\n",
        "    y_test, y_pred, kind=\"actual_vs_predicted\", ax=ax, scatter_kwargs={\"alpha\": 0.5}\n",
        ")\n",
        "ax.set_title(\"Elastic-Net model, optimum regularization\")\n",
        "for name, score in scores.items():\n",
        "    ax.plot([], [], \" \", label=f\"{name}: {score}\")\n",
        "ax.legend(loc=\"upper left\")\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lvV0O5eToNr"
      },
      "source": [
        "### Task 2.2 Plot the models coefficients' variability across folds for the linear models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "sR0XGAQhToNr"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import RepeatedKFold, cross_validate\n",
        "feature_names = model_Ridge[:-1].get_feature_names_out()\n",
        "cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=0)\n",
        "cv_model = cross_validate(\n",
        "    None,\n",
        "    None,\n",
        "    None,\n",
        "    cv=cv,\n",
        "    return_estimator=True,\n",
        "    n_jobs=2,\n",
        ")\n",
        "coefs = pd.DataFrame(\n",
        "    [est[-1].regressor_.coef_ for est in cv_model[\"estimator\"]], columns=feature_names\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "id": "zGs32MKQA5tT",
        "outputId": "f01c4f56-c342-45aa-859c-5ff8beccb3f5"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(9, 7))\n",
        "sns.stripplot(data=coefs, orient=\"h\", palette=\"dark:k\", alpha=0.5)\n",
        "sns.boxplot(data=coefs, orient=\"h\", color=\"cyan\", saturation=0.5)\n",
        "plt.axvline(x=0, color=\".5\")\n",
        "plt.title(\"Coefficient importance and its variability\")\n",
        "plt.xlabel(\"Coefficient importance\")\n",
        "plt.suptitle(\"Ridge model, optimal regularization\")\n",
        "plt.subplots_adjust(left=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "uxqBSUy4A9_p"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import RepeatedKFold, cross_validate\n",
        "feature_names = model_Lasso[:-1].get_feature_names_out()\n",
        "cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=0)\n",
        "cv_model = cross_validate(\n",
        "    None,\n",
        "    None,\n",
        "    None,\n",
        "    cv=cv,\n",
        "    return_estimator=True,\n",
        "    n_jobs=2,\n",
        ")\n",
        "coefs = pd.DataFrame(\n",
        "    [est[-1].regressor_.coef_ for est in cv_model[\"estimator\"]], columns=feature_names\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "id": "9-TsheNNA9_p",
        "outputId": "bfedc768-e143-495c-ac17-154f96ffc220"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(9, 7))\n",
        "sns.stripplot(data=coefs, orient=\"h\", palette=\"dark:k\", alpha=0.5)\n",
        "sns.boxplot(data=coefs, orient=\"h\", color=\"cyan\", saturation=0.5)\n",
        "plt.axvline(x=0, color=\".5\")\n",
        "plt.title(\"Coefficient importance and its variability\")\n",
        "plt.xlabel(\"Coefficient importance\")\n",
        "plt.suptitle(\"Lasso model, optimal regularization\")\n",
        "plt.subplots_adjust(left=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "P1C5SS54A9_q"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import RepeatedKFold, cross_validate\n",
        "feature_names = None[:-1].get_feature_names_out()\n",
        "cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=0)\n",
        "cv_model = cross_validate(\n",
        "    None,\n",
        "    None,\n",
        "    None,\n",
        "    cv=cv,\n",
        "    return_estimator=True,\n",
        "    n_jobs=2,\n",
        ")\n",
        "coefs = pd.DataFrame(\n",
        "    [est[-1].regressor_.coef_ for est in cv_model[\"estimator\"]], columns=feature_names\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "id": "9q2wTOwYA9_q",
        "outputId": "795729bc-c801-43cf-ea3e-6c156245808a"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(9, 7))\n",
        "sns.stripplot(data=coefs, orient=\"h\", palette=\"dark:k\", alpha=0.5)\n",
        "sns.boxplot(data=coefs, orient=\"h\", color=\"cyan\", saturation=0.5)\n",
        "plt.axvline(x=0, color=\".5\")\n",
        "plt.title(\"Coefficient importance and its variability\")\n",
        "plt.xlabel(\"Coefficient importance\")\n",
        "plt.suptitle(\"Elastic-net model, optimal regularization\")\n",
        "plt.subplots_adjust(left=0.3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwJR4MyGToNr"
      },
      "source": [
        "Discussion: Are the coefficents across the different models similar?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkTgdVizToNr"
      },
      "source": [
        "### Task 2.3 Plot the permutation feature importance for the different models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "t9qtkFVbHgdg",
        "outputId": "e6e0e25c-8ea6-443c-8b80-9578a546aa2e"
      },
      "outputs": [],
      "source": [
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "feature_names = X_test.columns if hasattr(X_test, 'columns') else [f\"feature {i}\" for i in range(X_test.shape[1])]\n",
        "\n",
        "result = permutation_importance(\n",
        "    None\n",
        ")\n",
        "\n",
        "forest_importances = pd.Series(result.importances_mean, index=feature_names)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "forest_importances.plot.bar(yerr=result.importances_std, ax=ax)\n",
        "ax.set_title(\"Feature importances using permutation on full model - Ridge\")\n",
        "ax.set_ylabel(\"Mean accuracy decrease\")\n",
        "fig.tight_layout()\n",
        "\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "lOd9aFdcHNva",
        "outputId": "16c29e12-7445-4dd3-bac1-28d36fc7d4df"
      },
      "outputs": [],
      "source": [
        "feature_names = X_test.columns if hasattr(X_test, 'columns') else [f\"feature {i}\" for i in range(X_test.shape[1])]\n",
        "\n",
        "result = permutation_importance(\n",
        "    None\n",
        ")\n",
        "\n",
        "forest_importances = pd.Series(None, index=None)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "forest_importances.plot.bar(yerr=result.importances_std, ax=ax)\n",
        "ax.set_title(\"Feature importances using permutation on full model - Lasso\")\n",
        "ax.set_ylabel(\"Mean accuracy decrease\")\n",
        "fig.tight_layout()\n",
        "\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "L1m8L16uGL-g",
        "outputId": "2b121bc0-38c3-4bbc-d582-23e64cfb4ab1"
      },
      "outputs": [],
      "source": [
        "feature_names = X_test.columns if hasattr(X_test, 'columns') else [f\"feature {i}\" for i in range(X_test.shape[1])]\n",
        "\n",
        "result = permutation_importance(\n",
        "    None\n",
        ")\n",
        "\n",
        "forest_importances = pd.Series(None)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "forest_importances.plot.bar(yerr=result.importances_std, ax=ax)\n",
        "ax.set_title(\"Feature importances using permutation on full model - Elastic Net\")\n",
        "ax.set_ylabel(\"Mean accuracy decrease\")\n",
        "fig.tight_layout()\n",
        "\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycu4ELxkToNs"
      },
      "source": [
        "Discussion: Are the feature coefficients simimar to the permutation importance for the different models?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoX5Z_1FToNs"
      },
      "source": [
        "### Task 2.4 Implement a similar pipeline for tree-based models and use the pipeline with random-forest and boosted regression trees to predict the wages from the other features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "LV-pOg9eIThB",
        "outputId": "4fcb44c5-437a-44bf-98cf-3fe0c4c15fa6"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.compose import TransformedTargetRegressor\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "\n",
        "# Random Forest Model\n",
        "rf_model = make_pipeline(\n",
        "    None,\n",
        "    TransformedTargetRegressor(\n",
        "        None\n",
        "    ),\n",
        ")\n",
        "\n",
        "rf_model.fit(None)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "E6oJVLBPIoL7",
        "outputId": "a9ce8f07-19d9-4e03-9d7c-04491433785c"
      },
      "outputs": [],
      "source": [
        "# Gradient Boosting Model\n",
        "gb_model = make_pipeline(\n",
        "    None,\n",
        ")\n",
        "gb_model.fit(None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kf2ZA0E4ToNs"
      },
      "source": [
        "### Task 2.5 Check the performance of the tree-based models plotting its predictions on the test set and computing the median absolute error of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "nfP7iePHToNs",
        "outputId": "0f40b062-a46c-4b1d-de24-f99d85a021c1"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import PredictionErrorDisplay, median_absolute_error\n",
        "\n",
        "mae_train = median_absolute_error(None)\n",
        "y_pred = None\n",
        "mae_test = None\n",
        "scores = {\n",
        "    \"MedAE on training set\": f\"{mae_train:.2f} $/hour\",\n",
        "    \"MedAE on testing set\": f\"{mae_test:.2f} $/hour\",\n",
        "}\n",
        "\n",
        "_, ax = plt.subplots(figsize=(5, 5))\n",
        "display = PredictionErrorDisplay.from_predictions(\n",
        "    y_test, y_pred, kind=\"actual_vs_predicted\", ax=ax, scatter_kwargs={\"alpha\": 0.5}\n",
        ")\n",
        "ax.set_title(\"Random Forest model, fixed parameters\")\n",
        "for name, score in scores.items():\n",
        "    ax.plot([], [], \" \", label=f\"{name}: {score}\")\n",
        "ax.legend(loc=\"upper left\")\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "h0ya87chI1kb",
        "outputId": "6dfe4585-5943-4ea5-e0e5-5a89904cd896"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import PredictionErrorDisplay, median_absolute_error\n",
        "\n",
        "mae_train = None\n",
        "mae_test = None\n",
        "scores = {\n",
        "    \"MedAE on training set\": f\"{mae_train:.2f} $/hour\",\n",
        "    \"MedAE on testing set\": f\"{mae_test:.2f} $/hour\",\n",
        "}\n",
        "\n",
        "_, ax = plt.subplots(figsize=(5, 5))\n",
        "display = PredictionErrorDisplay.from_predictions(\n",
        "    y_test, y_pred, kind=\"actual_vs_predicted\", ax=ax, scatter_kwargs={\"alpha\": 0.5}\n",
        ")\n",
        "ax.set_title(\"Gradient Boosting model, fixed parameters\")\n",
        "for name, score in scores.items():\n",
        "    ax.plot([], [], \" \", label=f\"{name}: {score}\")\n",
        "ax.legend(loc=\"upper left\")\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kKckTHiToNs"
      },
      "source": [
        "### Task 2.6 Plot the feature importance for the different tree-based models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "yKhZrLtfhAQD",
        "outputId": "a5e752a5-8f8c-4517-9e4f-d4731726c2fb"
      },
      "outputs": [],
      "source": [
        "# Access the RandomForestRegressor object inside the TransformedTargetRegressor which is inside the pipeline\n",
        "random_forest_regressor = rf_model.named_steps['transformedtargetregressor'].regressor_\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = random_forest_regressor.None\n",
        "\n",
        "\n",
        "feature_names = preprocessor.get_feature_names_out()\n",
        "\n",
        "\n",
        "# Create a pandas series with feature importances\n",
        "importances_series = pd.Series(feature_importances, index=feature_names)\n",
        "\n",
        "# Plot\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "importances_series.sort_values().plot.barh(ax=ax)\n",
        "ax.set_title(\"Feature Importance - Random Forest\")\n",
        "ax.set_xlabel(\"Importance\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "It6_hjh7Jvxs",
        "outputId": "087848e8-058a-4200-f4c6-f3e70aecf5a8"
      },
      "outputs": [],
      "source": [
        "# Access the RandomForestRegressor object inside the TransformedTargetRegressor which is inside the pipeline\n",
        "random_forest_regressor = gb_model.named_steps['transformedtargetregressor'].regressor_\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = random_forest_regressor.None\n",
        "\n",
        "\n",
        "feature_names = preprocessor.get_feature_names_out()\n",
        "\n",
        "\n",
        "# Create a pandas series with feature importances\n",
        "importances_series = pd.Series(feature_importances, index=feature_names)\n",
        "\n",
        "# Plot\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "importances_series.sort_values().plot.barh(ax=ax)\n",
        "ax.set_title(\"Feature Importance - Gradient Bootsing\")\n",
        "ax.set_xlabel(\"Importance\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7O5LF5NToNs"
      },
      "source": [
        "### Task 2.7 Plot the permutation feature importance for the different tree-based models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "oFDzDpLFToNs",
        "outputId": "543595bb-627f-4324-c543-02de702da3f7"
      },
      "outputs": [],
      "source": [
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "\n",
        "feature_names = X_test.columns if hasattr(X_test, 'columns') else [f\"feature {i}\" for i in range(X_test.shape[1])]\n",
        "\n",
        "result = permutation_importance(\n",
        "    None\n",
        ")\n",
        "\n",
        "forest_importances = pd.Series(result.importances_mean, index=feature_names)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "forest_importances.plot.bar(yerr=result.importances_std, ax=ax)\n",
        "ax.set_title(\"Feature importances using permutation on full model - Random Forest\")\n",
        "ax.set_ylabel(\"Mean accuracy decrease\")\n",
        "fig.tight_layout()\n",
        "\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "q3gUvWyLH8L0",
        "outputId": "097124d0-e7a6-4300-a58c-34cad0f2a13e"
      },
      "outputs": [],
      "source": [
        "\n",
        "feature_names = X_test.columns if hasattr(X_test, 'columns') else [f\"feature {i}\" for i in range(X_test.shape[1])]\n",
        "\n",
        "result = permutation_importance(\n",
        "    None\n",
        ")\n",
        "\n",
        "forest_importances = pd.Series(result.importances_mean, index=feature_names)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "forest_importances.plot.bar(yerr=result.importances_std, ax=ax)\n",
        "ax.set_title(\"Feature importances using permutation on full model - Gradient Boosting\")\n",
        "ax.set_ylabel(\"Mean accuracy decrease\")\n",
        "fig.tight_layout()\n",
        "\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVTTWpnUToNs"
      },
      "source": [
        "Discussion: Are the feature importance and permutation feature importance similar for the different models?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFKo9mhlToNt"
      },
      "source": [
        "### Task 2.8  For the best tree-based model use partial dependence plot to investigate dependence between the target response and each feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXO8NBfFQzzL",
        "outputId": "dbfcf85d-c9de-4108-dbba-2f6f1cc2c062"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import r2_score, mean_absolute_error\n",
        "\n",
        "# Predictions from Gradient Boosting model\n",
        "y_pred_gb = gb_model.predict(X_test)\n",
        "\n",
        "# Predictions from Random Forest model\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "\n",
        "# Compute R¬≤ score\n",
        "r2_gb = r2_score(y_test, y_pred_gb)\n",
        "r2_rf = r2_score(y_test, y_pred_rf)\n",
        "\n",
        "# Compute Mean Absolute Error\n",
        "mae_gb = mean_absolute_error(y_test, y_pred_gb)\n",
        "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
        "\n",
        "print(f\"Gradient Boosting R¬≤ Score: {r2_gb:.4f}\")\n",
        "print(f\"Random Forest R¬≤ Score: {r2_rf:.4f}\")\n",
        "\n",
        "print(f\"Gradient Boosting MAE: {mae_gb:.4f} $/hour\")\n",
        "print(f\"Random Forest MAE: {mae_rf:.4f} $/hour\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "RmBjDBimrD0e"
      },
      "outputs": [],
      "source": [
        "# refer to documentation ... create list of categories which signal which features are categorical\n",
        "\n",
        "# Get the list of all feature names\n",
        "feature_names = X_train.columns.tolist()\n",
        "\n",
        "# Get the list of numeric feature names\n",
        "numeric_feature_names = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "# Initialize an empty list to store the boolean values\n",
        "is_categorical = []\n",
        "\n",
        "# Iterate over all feature names\n",
        "for feature in feature_names:\n",
        "    # If the feature is not in the list of numeric features, it is categorical\n",
        "    if feature not in numeric_feature_names:\n",
        "        is_categorical.append(None)\n",
        "    else:\n",
        "        is_categorical.append(None)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wKmhEj5JpjPX",
        "outputId": "f9953bc5-e5bf-4287-b6fa-41011f4f7df0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.inspection import PartialDependenceDisplay\n",
        "\n",
        "# Generate partial dependence plots for selected features using the entire pipeline\n",
        "fig, ax = plt.subplots(figsize=(12, 10))  # Adjusted figure size\n",
        "PartialDependenceDisplay.from_estimator(None)\n",
        "\n",
        "plt.xticks(rotation=45)  # Rotate x-axis labels if needed\n",
        "plt.subplots_adjust(bottom=0.2, hspace=1, wspace=0.4)  # Adjust spacing\n",
        "plt.tight_layout()  # Adjust layout\n",
        "\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 696
        },
        "id": "1CrlZkmLr8Ml",
        "outputId": "de239120-111b-47cb-82f8-c5966d6ad739"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.inspection import PartialDependenceDisplay\n",
        "\n",
        "# Individual (only for numeric)\n",
        "\n",
        "# Generate partial dependence plots for selected features using the entire pipeline\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "PartialDependenceDisplay.from_estimator(None)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nL4jXNqvToNt"
      },
      "source": [
        "## Task 3: Include feature selection within the cross-validation pipeline implemented in Task 1 and try two different feature selection strategies (select k best and recursive feature elimination) with the ridge regression model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "Lpk-Vs1oToNt",
        "outputId": "1885e44b-3c42-49a3-8f2b-4b89be145fa5"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import RFECV,RFE,SelectKBest\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "\n",
        "alphas = np.logspace(-10, 10, 21)  # alpha values to be chosen from by cross-validation\n",
        "model = make_pipeline(\n",
        "    None\n",
        ")\n",
        "# selector = RFECV(model, cv=5)\n",
        "model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBYeVuoCToNt"
      },
      "source": [
        "### Task 3.1 Check the performance of the computed models plotting its predictions on the test set and computing the median absolute error of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "_o79CeqRToNt",
        "outputId": "79d1cc0e-a92a-40cb-bbe2-5fda06748d78"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import PredictionErrorDisplay, median_absolute_error\n",
        "\n",
        "mae_train = None\n",
        "y_pred = None\n",
        "mae_test = None\n",
        "scores = {\n",
        "    \"MedAE on training set\": f\"{mae_train:.2f} $/hour\",\n",
        "    \"MedAE on testing set\": f\"{mae_test:.2f} $/hour\",\n",
        "}\n",
        "\n",
        "_, ax = plt.subplots(figsize=(5, 5))\n",
        "display = PredictionErrorDisplay.from_predictions(\n",
        "    y_test, y_pred, kind=\"actual_vs_predicted\", ax=ax, scatter_kwargs={\"alpha\": 0.5}\n",
        ")\n",
        "ax.set_title(\"Ridge model + RFE and optimum regularization\")\n",
        "for name, score in scores.items():\n",
        "    ax.plot([], [], \" \", label=f\"{name}: {score}\")\n",
        "ax.legend(loc=\"upper left\")\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAAlj1PRToNt"
      },
      "source": [
        "Discussion: Did the model performance improved with feature selection?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-J27zzgToNu"
      },
      "source": [
        "### Task 3.2 Plot the coefficients variability across folds for the linear model based on the selected features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpewjP15ToNu"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import RepeatedKFold, cross_validate\n",
        "feature_names = model[:-1].get_feature_names_out()\n",
        "cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=0)\n",
        "cv_model = cross_validate(\n",
        "    None\n",
        ")\n",
        "coefs = pd.DataFrame(\n",
        "    [est[-1].regressor_.coef_ for est in cv_model[\"estimator\"]], columns=feature_names\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "id": "0zUsNO-JdRNo",
        "outputId": "babbbf8a-cb68-4dc2-b763-d80fadffd5f3",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(9, 7))\n",
        "sns.stripplot(data=coefs, orient=\"h\", palette=\"dark:k\", alpha=0.5)\n",
        "sns.boxplot(data=coefs, orient=\"h\", color=\"cyan\", saturation=0.5)\n",
        "plt.axvline(x=0, color=\".5\")\n",
        "plt.title(\"Coefficient importance and its variability\")\n",
        "plt.xlabel(\"Coefficient importance\")\n",
        "plt.suptitle(\"Ridge model  + RFE and optimum regularization\")\n",
        "plt.subplots_adjust(left=0.3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haLNCVvgToNu"
      },
      "source": [
        "Discussion: Are similar features selected using the different strategies?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "c975486aa82b32d30c2438da9d14334177c2b4d93822b75ed42b1917e361f4e6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
