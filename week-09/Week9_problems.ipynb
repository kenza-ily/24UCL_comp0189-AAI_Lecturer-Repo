{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4660a734",
      "metadata": {
        "id": "4660a734"
      },
      "source": [
        "# COMP0189: Applied Artificial Intelligence\n",
        "## Week 9 (Clustering)\n",
        "\n",
        "\n",
        "### ðŸŽ¯ Objectives\n",
        "1. To learn how to apply different clustering approaches (K-Means and Gaussian Mixture Models) to different tasks: clustering images and clustering voxels (image segmentation)\n",
        "2. To learn how to quantify clustering results\n",
        "\n",
        "\n",
        "\n",
        "### Acknowledgements\n",
        "- Many thanks to Prof. John Ashburner for kindly providing the brain imaging data.\n",
        "- https://scikit-learn.org/stable/\n",
        "- https://en.wikipedia.org/wiki/MNIST_database\n",
        "- https://brain-development.org/ixi-dataset/\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "25f55e3b",
      "metadata": {
        "id": "25f55e3b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11b18ed1",
      "metadata": {
        "id": "11b18ed1"
      },
      "source": [
        "# Part 1: MNIST dataset\n",
        "\n",
        "In this part we will apply Principal Component Analysis (PCA) to the MNIST dataset and use K-Means to cluster the digits.\n",
        "\n",
        "https://scikit-learn.org/stable/modules/clustering.html#k-means"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc8026c4",
      "metadata": {
        "id": "fc8026c4"
      },
      "source": [
        "### Task 1: Load MNIST data and assemble it in two matrices X (images) and y (labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c691c6a",
      "metadata": {
        "id": "7c691c6a",
        "outputId": "23b22d90-ef73-46f3-8f01-374b2a0daa11"
      },
      "outputs": [],
      "source": [
        "MNIST = np.load(\"mnist.npz\")\n",
        "for k in MNIST.files:\n",
        "    print(k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdc88b6a",
      "metadata": {
        "id": "fdc88b6a",
        "outputId": "9499e76a-ba90-4c02-d800-75b8baa22473"
      },
      "outputs": [],
      "source": [
        "MNIST[\"X\"].shape, MNIST[\"y\"].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a136cfd8",
      "metadata": {
        "id": "a136cfd8"
      },
      "outputs": [],
      "source": [
        "mnist_X = MNIST[None]\n",
        "mnist_y = MNIST[None]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07fa23e5",
      "metadata": {
        "id": "07fa23e5"
      },
      "source": [
        "### Task 2: Visualise the data for better understanding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bd01645",
      "metadata": {
        "id": "2bd01645",
        "outputId": "6d9c7d24-346b-450f-efd8-b3dea4f98685"
      },
      "outputs": [],
      "source": [
        "def make_img_grid(images, n_cols=10):\n",
        "    \"\"\"Helper function for arranging images into a grid\"\"\"\n",
        "    cols = []\n",
        "    gap = len(images) % n_cols\n",
        "    if gap > 0:\n",
        "        # add padding if needed\n",
        "        images = np.concatenate(\n",
        "            (images, np.zeros((n_cols - gap,) + images[0].shape)), 0\n",
        "        )\n",
        "    for n in range(n_cols):\n",
        "        cols.append(np.concatenate(images[np.arange(n, len(images), step=n_cols)]))\n",
        "    return np.concatenate(cols, -1)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(14, 7))\n",
        "plt.imshow(make_img_grid(mnist_X[:None], n_cols=30), cmap=\"binary_r\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36617b00",
      "metadata": {
        "id": "36617b00",
        "outputId": "99eaf64c-bb7b-4067-b122-d542b30ef1f2"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(3, 3, figsize=(12, 12))\n",
        "plt.gray()\n",
        "\n",
        "# loop through subplots and add mnist images\n",
        "for i, ax in enumerate(axs.flat):\n",
        "    ax.imshow(mnist_X[None])\n",
        "    ax.axis(\"off\")\n",
        "    ax.set_title(\"Number {}\".format(mnist_y[i]))\n",
        "\n",
        "# display the figure\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea624926",
      "metadata": {
        "id": "ea624926"
      },
      "source": [
        "In order to apply PCA to the MNIST data we need to reshape the original MNIST data, from\n",
        "\n",
        "    mnist_images.shape == [60000, 28, 28]\n",
        "\n",
        "into a 2d array (matrix)\n",
        "\n",
        "    X_mnist.shape == [60000, 784]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dd456e9",
      "metadata": {
        "id": "8dd456e9",
        "outputId": "56762ba8-f47d-41b6-97e9-5aaeaca12e55"
      },
      "outputs": [],
      "source": [
        "# Reshaping\n",
        "mnist_X_original = mnist_X.copy()\n",
        "mnist_X = mnist_X.reshape(len(mnist_X), -1)\n",
        "print(mnist_X.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71b93f31",
      "metadata": {
        "id": "71b93f31"
      },
      "source": [
        "### Task 3: Apply PCA to the MNIST data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44615f96",
      "metadata": {
        "id": "44615f96"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA()\n",
        "mnist_X_pca = pca.None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d645557e",
      "metadata": {
        "id": "d645557e",
        "outputId": "1a23da59-53d6-45af-87a2-b5d303871c52"
      },
      "outputs": [],
      "source": [
        "mnist_X_pca.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aad1f0b4",
      "metadata": {
        "id": "aad1f0b4"
      },
      "source": [
        "### Task 4: Plot the MNIST data projected onto the first two principal components (using different colours for the different digits). Use the labels to colour the examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "338836aa",
      "metadata": {
        "id": "338836aa",
        "outputId": "541e0f94-ba93-4d0b-f2b3-973a1ee2ff94"
      },
      "outputs": [],
      "source": [
        "plot = plt.scatter(mnist_X_pca[:, None], mnist_X_pca[:, None], c=mnist_y, cmap=\"rainbow\")\n",
        "plt.legend(handles=plot.legend_elements()[0], labels=[x for x in range(10)])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bdc9771",
      "metadata": {
        "id": "0bdc9771"
      },
      "source": [
        "### Task 5: Plot the explained variance per component"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23af3e90",
      "metadata": {
        "id": "23af3e90",
        "outputId": "b2f16f5c-dc44-4f13-8989-ad0bb0a7b399"
      },
      "outputs": [],
      "source": [
        "# Determine explained variance using explained_variance_ratio_ attribute\n",
        "# explained_variance_ratio_\n",
        "#   Percentage of variance explained by each of the selected components.\n",
        "#   If n_components is not set then all components are stored and the sum of the ratios is equal to 1.0.\n",
        "exp_var_pca = pca.None  # shape: (784,)\n",
        "\n",
        "# Cumulative sum of eigenvalues will be used to create step plot\n",
        "# for visualizing the variance explained by each principal component.\n",
        "cum_sum_eigenvalues = np.cumsum(exp_var_pca)\n",
        "\n",
        "# Create the plot\n",
        "plt.bar(\n",
        "    range(0, len(exp_var_pca)),\n",
        "    exp_var_pca,\n",
        "    alpha=0.5,\n",
        "    align=\"center\",\n",
        "    label=\"Individual explained variance\",\n",
        ")\n",
        "plt.step(\n",
        "    range(0, len(cum_sum_eigenvalues)),\n",
        "    cum_sum_eigenvalues,\n",
        "    where=\"mid\",\n",
        "    label=\"Cumulative explained variance\",\n",
        ")\n",
        "plt.ylabel(\"Explained variance ratio\")\n",
        "plt.xlabel(\"Principal component index\")\n",
        "plt.legend(loc=\"best\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fe72d87",
      "metadata": {
        "id": "5fe72d87"
      },
      "source": [
        "### Manual way of gettting explained variance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a71d7f08",
      "metadata": {
        "id": "a71d7f08"
      },
      "outputs": [],
      "source": [
        "def compute_PCA_parameters(X, M):\n",
        "    \"\"\"\n",
        "    This function computes the first M prinicpal components of a\n",
        "    dataset X. It returns the mean of the data, the projection matrix,\n",
        "    and the associated singular values.\n",
        "\n",
        "    While you can compute this however you want, `np.linalg.svd` is\n",
        "    highly recommended. Please look at its documentation to choose\n",
        "    its arguments appropriately, and on how to interpret its return values.\n",
        "\n",
        "    INPUT:\n",
        "    X    : (N, D) matrix; each row is a D-dimensional data point\n",
        "    M    : integer, <= D (number of principal components to return)\n",
        "\n",
        "    OUTPUTS:\n",
        "    x_bar  : (D,) vector, with the mean of the data\n",
        "    W      : (D, M) semi-orthogonal matrix of projections\n",
        "    s      : (D,) vector of singular values\n",
        "    \"\"\"\n",
        "    N, D = X.shape\n",
        "    x_bar = np.mean(X, 0)\n",
        "    X_bar = X - x_bar\n",
        "    u, s, vh = np.linalg.svd(X_bar, full_matrices=False)\n",
        "    W = vh.T[:, :M]\n",
        "    return x_bar, W, s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "957ad3cb",
      "metadata": {
        "id": "957ad3cb",
        "outputId": "f7b0dfdc-52dc-4528-cc65-1ed6ab9d403c"
      },
      "outputs": [],
      "source": [
        "mnist_mean, W_mnist, s_mnist = compute_PCA_parameters(mnist_X, 50)\n",
        "\n",
        "N, data_dim = mnist_X.shape\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.plot(np.arange(data_dim) + 1, s_mnist**2 / N, \".-\")\n",
        "plt.xlabel(\"Component\")\n",
        "plt.ylabel(\"Explained variance\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60b0ac98",
      "metadata": {
        "id": "60b0ac98"
      },
      "source": [
        "### Task 5: Apply KMeans to cluster the MNIST data\n",
        "\n",
        "- Preprocess the MNIST dataset with PCA to compress it down to a 2-dimensional feature space before applying the K-Means\n",
        "\n",
        "- Try K-Means with different numbers of clusters and use the Silhouette Coefficient to choose the optimal number of cluster\n",
        "\n",
        "Discussion: Does the Silhouette Coefficient chooses the right number of clusters?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67696103",
      "metadata": {
        "id": "67696103",
        "outputId": "63de488e-5562-4e88-b772-309edeed4f54"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "pca = PCA(n_components=None)\n",
        "mnist_X_pca = pca.None\n",
        "n_digits = len(np.unique(mnist_y))\n",
        "print(n_digits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40913276",
      "metadata": {
        "id": "40913276",
        "outputId": "93d220d3-62b0-4615-bf95-6218b0e5e55e"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "kmeans = KMeans(n_clusters=None, n_init=10)\n",
        "kmeans.fit(None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49a7c0a5",
      "metadata": {
        "id": "49a7c0a5",
        "outputId": "5c5762c2-92e0-4e6b-c03b-433688f5195d"
      },
      "outputs": [],
      "source": [
        "import matplotlib.cm as cm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "\n",
        "# Generating the sample data from make_blobs\n",
        "# This particular setting has one distinct cluster and 3 clusters placed close\n",
        "# together.\n",
        "X = None\n",
        "y = None\n",
        "range_n_clusters = [2, 5,10]\n",
        "\n",
        "for n_clusters in range_n_clusters:\n",
        "    # Create a subplot with 1 row and 2 columns\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "    fig.set_size_inches(18, 7)\n",
        "\n",
        "    # The 1st subplot is the silhouette plot\n",
        "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
        "    # lie within [-0.1, 1]\n",
        "    ax1.set_xlim([-0.1, 1])\n",
        "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
        "    # plots of individual clusters, to demarcate them clearly.\n",
        "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
        "\n",
        "    # Initialize the clusterer with n_clusters value and a random generator\n",
        "    # seed of 10 for reproducibility.\n",
        "    clusterer = KMeans(n_clusters=n_clusters,  n_init=10 , random_state=10)\n",
        "    cluster_labels = clusterer.fit_predict(X)\n",
        "\n",
        "    # The silhouette_score gives the average value for all the samples.\n",
        "    # This gives a perspective into the density and separation of the formed\n",
        "    # clusters\n",
        "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
        "    print(\n",
        "        \"For n_clusters =\",\n",
        "        n_clusters,\n",
        "        \"The average silhouette_score is :\",\n",
        "        silhouette_avg,\n",
        "    )\n",
        "\n",
        "    # Compute the silhouette scores for each sample\n",
        "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
        "\n",
        "    y_lower = 10\n",
        "    for i in range(n_clusters):\n",
        "        # Aggregate the silhouette scores for samples belonging to\n",
        "        # cluster i, and sort them\n",
        "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
        "\n",
        "        ith_cluster_silhouette_values.sort()\n",
        "\n",
        "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "        y_upper = y_lower + size_cluster_i\n",
        "\n",
        "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
        "        ax1.fill_betweenx(\n",
        "            np.arange(y_lower, y_upper),\n",
        "            0,\n",
        "            ith_cluster_silhouette_values,\n",
        "            facecolor=color,\n",
        "            edgecolor=color,\n",
        "            alpha=0.7,\n",
        "        )\n",
        "\n",
        "        # Label the silhouette plots with their cluster numbers at the middle\n",
        "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
        "\n",
        "        # Compute the new y_lower for next plot\n",
        "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
        "\n",
        "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
        "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
        "    ax1.set_ylabel(\"Cluster label\")\n",
        "\n",
        "    # The vertical line for average silhouette score of all the values\n",
        "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
        "\n",
        "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
        "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
        "\n",
        "    # 2nd Plot showing the actual clusters formed\n",
        "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
        "    ax2.scatter(\n",
        "        X[:, 0], X[:, 1], marker=\".\", s=30, lw=0, alpha=0.7, c=colors, edgecolor=\"k\"\n",
        "    )\n",
        "\n",
        "    # Labeling the clusters\n",
        "    centers = clusterer.cluster_centers_\n",
        "    # Draw white circles at cluster centers\n",
        "    ax2.scatter(\n",
        "        centers[:, 0],\n",
        "        centers[:, 1],\n",
        "        marker=\"o\",\n",
        "        c=\"white\",\n",
        "        alpha=1,\n",
        "        s=200,\n",
        "        edgecolor=\"k\",\n",
        "    )\n",
        "\n",
        "    for i, c in enumerate(centers):\n",
        "        ax2.scatter(c[0], c[1], marker=\"$%d$\" % i, alpha=1, s=50, edgecolor=\"k\")\n",
        "\n",
        "    ax2.set_title(\"The visualization of the clustered data.\")\n",
        "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
        "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
        "\n",
        "    plt.suptitle(\n",
        "        \"Silhouette analysis for KMeans clustering on sample data with n_clusters = %d\"\n",
        "        % n_clusters,\n",
        "        fontsize=14,\n",
        "        fontweight=\"bold\",\n",
        "    )\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2386d890",
      "metadata": {
        "id": "2386d890"
      },
      "source": [
        "### Task 6: Apply the K-Means to the MNIST dataset compressed to 2, 10 and 100 PCs (fixing the number of clusters to 10) and quantify the performance of the three cluster models using different clustering metrics\n",
        "\n",
        "See \"Quantifying the quality of clustering results\" in https://scikit-learn.org/stable/auto_examples/text/plot_document_clustering.html#sphx-glr-auto-examples-text-plot-document-clustering-py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1db5c4a2",
      "metadata": {
        "id": "1db5c4a2"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "from time import time\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "evaluations = []\n",
        "evaluations_std = []\n",
        "labels = mnist_y\n",
        "\n",
        "def fit_and_evaluate(km, X, name=None, n_runs=5):\n",
        "    name = km.__class__.__name__ if name is None else name\n",
        "\n",
        "    train_times = []\n",
        "    scores = defaultdict(list)\n",
        "    for seed in range(n_runs):\n",
        "        km.set_params(random_state=seed)\n",
        "        t0 = time()\n",
        "        km.fit(X)\n",
        "        train_times.append(time() - t0)\n",
        "        scores[\"Homogeneity\"].append(metrics.homogeneity_score(labels, km.labels_))\n",
        "        scores[\"Completeness\"].append(metrics.completeness_score(labels, km.labels_))\n",
        "        scores[\"V-measure\"].append(metrics.v_measure_score(labels, km.labels_))\n",
        "        scores[\"Adjusted Rand-Index\"].append(\n",
        "            metrics.adjusted_rand_score(labels, km.labels_)\n",
        "        )\n",
        "        scores[\"Silhouette Coefficient\"].append(\n",
        "            metrics.silhouette_score(X, km.labels_, sample_size=2000)\n",
        "        )\n",
        "    train_times = np.asarray(train_times)\n",
        "\n",
        "    print(f\"clustering done in {train_times.mean():.2f} Â± {train_times.std():.2f} s \")\n",
        "    evaluation = {\n",
        "        \"estimator\": name,\n",
        "        \"train_time\": train_times.mean(),\n",
        "    }\n",
        "    evaluation_std = {\n",
        "        \"estimator\": name,\n",
        "        \"train_time\": train_times.std(),\n",
        "    }\n",
        "    for score_name, score_values in scores.items():\n",
        "        mean_score, std_score = np.mean(score_values), np.std(score_values)\n",
        "        print(f\"{score_name}: {mean_score:.3f} Â± {std_score:.3f}\")\n",
        "        evaluation[score_name] = mean_score\n",
        "        evaluation_std[score_name] = std_score\n",
        "    evaluations.append(evaluation)\n",
        "    evaluations_std.append(evaluation_std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82fdcb3b",
      "metadata": {
        "id": "82fdcb3b",
        "outputId": "02cf086f-03ef-437b-c7ea-c16325eb26e3"
      },
      "outputs": [],
      "source": [
        "pca = PCA(n_components=None)\n",
        "mnist_X_pca = pca.None\n",
        "kmeans = KMeans(n_clusters=n_digits, n_init=10)\n",
        "fit_and_evaluate(\n",
        "    None,\n",
        "    None,\n",
        "    name=\"2-PCA-Kmeans\",\n",
        ")\n",
        "pca = PCA(n_components=10)\n",
        "mnist_X_pca = pca.None\n",
        "kmeans = KMeans(n_clusters=n_digits, n_init=10)\n",
        "fit_and_evaluate(\n",
        "    None,\n",
        "    None,\n",
        "    name=\"10-PCA-Kmeans\",\n",
        ")\n",
        "\n",
        "pca = PCA(n_components=100)\n",
        "mnist_X_pca = pca.None\n",
        "kmeans = KMeans(n_clusters=n_digits, n_init=10)\n",
        "fit_and_evaluate(\n",
        "    None,\n",
        "    None,\n",
        "    name=\"100-PCA-Kmeans\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "309d6168",
      "metadata": {
        "id": "309d6168",
        "outputId": "2eb16359-5cb2-4ad9-a9e2-04bd326d5c0e"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "fig, (ax0, ax1) = plt.subplots(ncols=2, figsize=(16, 6), sharey=True)\n",
        "\n",
        "df = pd.DataFrame(evaluations[::-1]).set_index(\"estimator\")\n",
        "df_std = pd.DataFrame(evaluations_std[::-1]).set_index(\"estimator\")\n",
        "\n",
        "df.drop(\n",
        "    [\"train_time\"],\n",
        "    axis=\"columns\",\n",
        ").plot.barh(ax=ax0, xerr=df_std)\n",
        "ax0.set_xlabel(\"Clustering scores\")\n",
        "ax0.set_ylabel(\"\")\n",
        "\n",
        "df[\"train_time\"].plot.barh(ax=ax1, xerr=df_std[\"train_time\"])\n",
        "ax1.set_xlabel(\"Clustering time (s)\")\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ca1ef44",
      "metadata": {
        "id": "0ca1ef44"
      },
      "source": [
        "# Part 2: IXI dataset\n",
        "\n",
        "In this part we apply use Gaussian Mixture Model (GMM) to an image segmentation task. We will use the voxel intensities of two brain images to cluster them into four different clusters that should correspond to different types of brain tissue (grey matter, white amtter, Cerebrospinal fluid (CSF) and \"other\").\n",
        "\n",
        "https://scikit-learn.org/stable/modules/mixture.html#gmm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hWdM-Op9khoF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWdM-Op9khoF",
        "outputId": "ded05661-0b57-47f2-ac83-f7c217984895"
      },
      "outputs": [],
      "source": [
        "!pip install nilearn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebf4e904",
      "metadata": {
        "id": "ebf4e904"
      },
      "source": [
        "### Task 8: Load two brain images (proton density-weighted (PDW) and T2-weighted (T2W)) that have been previously pre-processed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "4a002aab",
      "metadata": {
        "id": "4a002aab"
      },
      "outputs": [],
      "source": [
        "import nibabel\n",
        "f = nibabel.load(\"/content/T2.nii\")\n",
        "x1 = f.get_fdata()\n",
        "\n",
        "f = nibabel.load(\"/content/PD.nii\")\n",
        "x2 = f.get_fdata()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90fc247e",
      "metadata": {
        "id": "90fc247e"
      },
      "source": [
        "### Task 9: Vectorize the brain images and combine them to create a data matrix with 2 dimensions per voxel (PD and T2 intensities)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "914ab5dc",
      "metadata": {
        "id": "914ab5dc"
      },
      "outputs": [],
      "source": [
        "# Flatten the 3D arrays to 2D arrays where each row is a voxel\n",
        "x1_flat = x1.reshape(None, 1) # Reshape x1 to have one column and as many rows as there are voxels\n",
        "x2_flat = x2.reshape(None, 1) # Reshape x2 similarly\n",
        "\n",
        "# Combine the flattened arrays side by side to create a new 2D array with two columns (PD and T2 intensities)\n",
        "data_matrix = np.hstack((None))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_60o68S5jmPg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_60o68S5jmPg",
        "outputId": "00ad4269-d786-4b6b-e65a-a16ea76accb4"
      },
      "outputs": [],
      "source": [
        "print(data_matrix.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "Igqa6F4jUkWc",
      "metadata": {
        "id": "Igqa6F4jUkWc"
      },
      "outputs": [],
      "source": [
        "# reshaping to original\n",
        "original_shape = x1.shape\n",
        "\n",
        "# Split the 2D data_matrix back into two 1D arrays\n",
        "x_1_flat = data_matrix[:, 0]  # This extracts the first column (PD intensities)\n",
        "x_2_flat = data_matrix[:, 1]  # This extracts the second column (T2 intensities)\n",
        "\n",
        "# Reshape these 1D arrays back into their original 3D shape\n",
        "x1_reshaped = x_1_flat.reshape(None)\n",
        "x2_reshaped = x_2_flat.reshape(None)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SHxgVnJSuScu",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHxgVnJSuScu",
        "outputId": "b5dafdda-203c-46e6-be38-b9cee87cd77e"
      },
      "outputs": [],
      "source": [
        "print(x_1_flat.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UfWMDxcyUm9W",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfWMDxcyUm9W",
        "outputId": "b3fe4b83-6c28-435d-be2f-d6eb822f6f6e"
      },
      "outputs": [],
      "source": [
        "# check if original and un-reshape reshaped match\n",
        "print(np.array_equal(x1, x1_reshaped))\n",
        "print(np.array_equal(x2, x2_reshaped))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b7c43e2",
      "metadata": {
        "id": "8b7c43e2"
      },
      "source": [
        "###  Task 10: Apply GMM to cluster the voxels fixing the number of cluster to 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "a605147b",
      "metadata": {
        "id": "a605147b"
      },
      "outputs": [],
      "source": [
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "# Assuming data_matrix is the combined 2D array from the previous steps\n",
        "\n",
        "# Initialize the Gaussian Mixture Model with 4 components (clusters)\n",
        "gmm = GaussianMixture(n_components=None, random_state=0)\n",
        "\n",
        "# Fit the model to the data and predict the cluster for each voxel\n",
        "cluster_labels = gmm.fit_predict(None)\n",
        "\n",
        "# cluster_labels is now a 1D array with the cluster label (0, 1, 2, 3) for each voxel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sYrGJdn6jyqy",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYrGJdn6jyqy",
        "outputId": "034e4a98-89cd-495f-b111-152f967442e8"
      },
      "outputs": [],
      "source": [
        "print(cluster_labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d271a152",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d271a152",
        "outputId": "560e4454-2bc6-4ada-ae86-e6467ddf54ba"
      },
      "outputs": [],
      "source": [
        "# Reshape cluster_labels back to the original 3D shape of the images\n",
        "clustered_image = cluster_labels.reshape(None)\n",
        "\n",
        "# clustered_image is a 3D array with the same shape as the original images,\n",
        "# where each voxel's value represents its cluster label.\n",
        "print(clustered_image.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fBcbe0cjMUF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "1fBcbe0cjMUF",
        "outputId": "b4676c94-b8e9-402b-d632-e255e8ca8187"
      },
      "outputs": [],
      "source": [
        "import nibabel\n",
        "from nilearn import plotting\n",
        "\n",
        "# Convert the clustered image to a Nifti1Image object using the correct affine matrix\n",
        "clustered_img_nii = nibabel.Nifti1Image(clustered_image.astype(np.int16), affine=f.affine)\n",
        "\n",
        "# View the clustered image in an interactive viewer\n",
        "plotting.view_img(None, threshold='auto', cmap='nipy_spectral')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JclJoAyDxtys",
      "metadata": {
        "id": "JclJoAyDxtys"
      },
      "source": [
        "Alternative, instead of classes, based on probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "9OZhy_xRtUMm",
      "metadata": {
        "id": "9OZhy_xRtUMm"
      },
      "outputs": [],
      "source": [
        "# Predict the probabilities for each voxel belonging to each cluster\n",
        "voxel_probabilities = gmm.predict_proba(None)\n",
        "\n",
        "# Now, voxel_probabilities is a 2D numpy array where each row represents a voxel\n",
        "# and each column represents the probability of that voxel belonging to a certain cluster.\n",
        "\n",
        "# Reshape these probabilities back to the original 3D shape for each cluster\n",
        "probabilities_4d = np.zeros((x1.shape[0], x1.shape[1], x1.shape[2], gmm.n_components))\n",
        "\n",
        "for i in range(gmm.n_components):\n",
        "    # Reshape the probabilities for cluster i into the original 3D shape\n",
        "    probabilities_4d[:,:,:,i] = voxel_probabilities[:,i].reshape(x1.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qy9fLCYowcZO",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "qy9fLCYowcZO",
        "outputId": "c868a273-b855-4384-842a-ae94024bf6d1"
      },
      "outputs": [],
      "source": [
        "from nilearn import plotting\n",
        "\n",
        "# Convert the probability maps to NIfTI images and visualize\n",
        "prob_img_nii = nibabel.Nifti1Image(probabilities_4d[:,:,:,None].astype(np.float32), affine=f.affine)\n",
        "plotting.view_img(prob_img_nii, threshold='auto', cmap='hot', title=f'Cluster {0} Probability Map')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IRTkILfbxDfk",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "IRTkILfbxDfk",
        "outputId": "339f55a9-4317-4f11-8752-4018a0fd7fad"
      },
      "outputs": [],
      "source": [
        "prob_img_nii = nibabel.Nifti1Image(probabilities_4d[:,:,:,None].astype(np.float32), affine=f.affine)\n",
        "plotting.view_img(prob_img_nii, threshold='auto', cmap='hot', title=f'Cluster {1} Probability Map')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-kckHQJDxj0q",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "-kckHQJDxj0q",
        "outputId": "7e4d9d05-5745-47d0-89c7-4256945bf2c5"
      },
      "outputs": [],
      "source": [
        "prob_img_nii = nibabel.Nifti1Image(probabilities_4d[:,:,:,None].astype(np.float32), affine=f.affine)\n",
        "plotting.view_img(prob_img_nii, threshold='auto', cmap='hot', title=f'Cluster {2} Probability Map')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ElqJWP1oyXCQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "ElqJWP1oyXCQ",
        "outputId": "89207df8-1734-4b4d-db02-83518555b39e"
      },
      "outputs": [],
      "source": [
        "prob_img_nii = nibabel.Nifti1Image(probabilities_4d[:,:,:,None].astype(np.float32), affine=f.affine)\n",
        "plotting.view_img(prob_img_nii, threshold='auto', cmap='hot', title=f'Cluster {2} Probability Map')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "L2eyRGZPpH-_",
      "metadata": {
        "id": "L2eyRGZPpH-_"
      },
      "source": [
        "#### As above but now ignoring the background class and using 3 clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Qn9oUt7UxqjK",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "Qn9oUt7UxqjK",
        "outputId": "8fcf3d90-0c97-4212-ec1d-50112353eb3d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import nibabel\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from nilearn import plotting\n",
        "\n",
        "# Load the images\n",
        "f_t2 = nibabel.load(\"/content/T2.nii\")\n",
        "x1 = f_t2.get_fdata()\n",
        "f_pd = nibabel.load(\"/content/PD.nii\")\n",
        "x2 = f_pd.get_fdata()\n",
        "\n",
        "# Flatten the 3D arrays to 2D arrays where each row is a voxel\n",
        "x1_flat = x1.reshape(-1, 1) # Reshape x1 to have one column and as many rows as there are voxels\n",
        "x2_flat = x2.reshape(-1, 1) # Reshape x2 similarly\n",
        "\n",
        "# Combine the flattened arrays side by side\n",
        "data_matrix = np.hstack((None))\n",
        "\n",
        "# Mask for non-zero pixels\n",
        "non_zero_mask = np.any(data_matrix != 0, axis=1)\n",
        "\n",
        "# Filter out the zero-valued pixels\n",
        "data_matrix_non_zero = data_matrix[None]\n",
        "\n",
        "# Initialize the Gaussian Mixture Model\n",
        "gmm = GaussianMixture(n_components=None, random_state=0)\n",
        "\n",
        "cluster_labels_non_zero = gmm.fit_predict(None)\n",
        "\n",
        "# Adjust the labels to start from 1 instead of 0 (for visualisaiton purposes)\n",
        "cluster_labels_non_zero += 1\n",
        "\n",
        "# Initialize an output array with zeros for the background\n",
        "clustered_image = np.zeros(x1_flat.shape[0], dtype=np.int16)\n",
        "\n",
        "# Assign the GMM cluster labels back to the non-zero pixels in the output array\n",
        "clustered_image[non_zero_mask] = cluster_labels_non_zero\n",
        "\n",
        "# Reshape cluster_labels back to the original 3D shape of the images\n",
        "clustered_image_reshaped = clustered_image.reshape(None)\n",
        "\n",
        "# Convert the clustered image to a Nifti1Image object using the correct affine matrix\n",
        "clustered_img_nii = nibabel.Nifti1Image(clustered_image_reshaped, affine=f_t2.affine)\n",
        "\n",
        "# View the clustered image\n",
        "plotting.view_img(None, threshold='auto', cmap='nipy_spectral')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "5qrV7OEIpVDh",
      "metadata": {
        "id": "5qrV7OEIpVDh"
      },
      "outputs": [],
      "source": [
        "# Predict the probabilities for each voxel belonging to each cluster\n",
        "voxel_probabilities = gmm.predict_proba(data_matrix_non_zero)\n",
        "\n",
        "# Initialize an output 4D array for probabilities with zeros\n",
        "# This array has the same width, height, and depth as the original images, and an extra dimension for each cluster\n",
        "probabilities_4d = np.zeros((x1.shape[0], x1.shape[1], x1.shape[2], gmm.n_components))\n",
        "\n",
        "# We need a way to map the probabilities back to their original positions including zeros\n",
        "# Flatten the 4D array to match the shape of the non-zero processing (x1_flat.shape[0], n_components)\n",
        "probabilities_flat = probabilities_4d.reshape(None, gmm.n_components)\n",
        "\n",
        "# Assign the probabilities back to the non-zero positions in the flattened array\n",
        "probabilities_flat[non_zero_mask, :] = voxel_probabilities\n",
        "\n",
        "# Reshape this flat probabilities array back to the original 4D shape\n",
        "probabilities_4d = probabilities_flat.reshape(x1.shape[0], x1.shape[1], x1.shape[2], gmm.n_components)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "i7nI2IAfph6A",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "i7nI2IAfph6A",
        "outputId": "996c6eaa-c6e0-4802-dd39-46e895e75c72"
      },
      "outputs": [],
      "source": [
        "from nilearn import plotting\n",
        "\n",
        "# Convert the probability maps to NIfTI images and visualize\n",
        "prob_img_nii = nibabel.Nifti1Image(probabilities_4d[:,:,:,None].astype(np.float32), affine=f.affine)\n",
        "plotting.view_img(prob_img_nii, threshold='auto', cmap='hot', title=f'Cluster {0} Probability Map')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iI2Ia2z6pkaR",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "iI2Ia2z6pkaR",
        "outputId": "0fbf1aa4-f8b5-404b-82d9-b3f4022e8bcf"
      },
      "outputs": [],
      "source": [
        "prob_img_nii = nibabel.Nifti1Image(probabilities_4d[:,:,:,None].astype(np.float32), affine=f.affine)\n",
        "plotting.view_img(prob_img_nii, threshold='auto', cmap='hot', title=f'Cluster {1} Probability Map')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "t-w3q9RzpnCN",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "t-w3q9RzpnCN",
        "outputId": "5661ed6b-f924-4f1e-c2d5-2de22585e530"
      },
      "outputs": [],
      "source": [
        "prob_img_nii = nibabel.Nifti1Image(probabilities_4d[:,:,:,None].astype(np.float32), affine=f.affine)\n",
        "plotting.view_img(prob_img_nii, threshold='auto', cmap='hot', title=f'Cluster {2} Probability Map')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "INlnEjgmthFp",
      "metadata": {
        "id": "INlnEjgmthFp"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "11b18ed1"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
